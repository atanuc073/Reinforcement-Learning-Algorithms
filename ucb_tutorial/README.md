# ğŸ° Upper Confidence Bound (UCB1) â€” A Complete Tutorial

## From Scratch: Multi-Armed Bandits, Exploration vs Exploitation, and the UCB1 Theorem

---

## Table of Contents

1. [The Multi-Armed Bandit Problem](#1-the-multi-armed-bandit-problem)
2. [Exploration vs Exploitation Dilemma](#2-exploration-vs-exploitation-dilemma)
3. [Naive Strategies and Their Flaws](#3-naive-strategies-and-their-flaws)
4. [Concentration Inequalities â€” The Mathematical Foundation](#4-concentration-inequalities--the-mathematical-foundation)
5. [The UCB1 Algorithm](#5-the-ucb1-algorithm)
6. [The UCB1 Theorem â€” Regret Bound](#6-the-ucb1-theorem--regret-bound)
7. [Step-by-Step Proof Sketch](#7-step-by-step-proof-sketch)
8. [Python Implementation](#8-python-implementation)
9. [Experiments and Visualization](#9-experiments-and-visualization)
10. [Comparison with Other Strategies](#10-comparison-with-other-strategies)
11. [Key Takeaways](#11-key-takeaways)

---

## 1. The Multi-Armed Bandit Problem

### The Casino Analogy

Imagine you walk into a casino with **K slot machines** (historically called "one-armed bandits").
Each machine has a **different, unknown** probability of paying out a reward.

Your **goal**: Maximize your total reward over **T** rounds of play.

The **catch**: You don't know which machine is the best! You must learn by playing.

```
    Machine 1       Machine 2       Machine 3       Machine 4
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ğŸ°    â”‚       â”‚ ğŸ°    â”‚       â”‚ ğŸ°    â”‚       â”‚ ğŸ°    â”‚
    â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
    â”‚ Î¼â‚=?  â”‚       â”‚ Î¼â‚‚=?  â”‚       â”‚ Î¼â‚ƒ=?  â”‚       â”‚ Î¼â‚„=?  â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”˜       â””â”€â”€â”€â”¬â”€â”€â”€â”˜       â””â”€â”€â”€â”¬â”€â”€â”€â”˜       â””â”€â”€â”€â”¬â”€â”€â”€â”˜
        â”‚               â”‚               â”‚               â”‚
     [Pull]          [Pull]          [Pull]          [Pull]
        â”‚               â”‚               â”‚               â”‚
        â–¼               â–¼               â–¼               â–¼
    Reward ~ Dâ‚     Reward ~ Dâ‚‚     Reward ~ Dâ‚ƒ     Reward ~ Dâ‚„
```

### Formal Definition

- **K arms** (actions), indexed by `i = 1, 2, ..., K`
- Each arm `i` has an **unknown reward distribution** with true mean **Î¼áµ¢**
- At each time step `t = 1, 2, ..., T`:
  - The agent **selects** an arm `Aâ‚œ âˆˆ {1, ..., K}`
  - The agent **receives** a reward `Xâ‚œ ~ Distribution(Î¼_{Aâ‚œ})`
- **Goal**: Maximize total expected reward `E[âˆ‘â‚œ Xâ‚œ]`

### Real-World Examples

| Domain | Arms | Reward |
|--------|------|--------|
| Clinical Trials | Different drugs | Patient recovery |
| Online Ads | Ad variants | Click-through rate |
| A/B Testing | Website designs | Conversion rate |
| News Recommendation | Articles | User engagement |
| Restaurant Selection | Restaurants | Meal satisfaction |

---

## 2. Exploration vs Exploitation Dilemma

This is the **central tension** of the bandit problem:

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     EXPLORATION vs EXPLOITATION          â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚   EXPLORE ğŸ”    â”‚    EXPLOIT ğŸ’°          â”‚
            â”‚                 â”‚                        â”‚
            â”‚ Try new/less-   â”‚ Play the arm with      â”‚
            â”‚ played arms to  â”‚ the highest observed   â”‚
            â”‚ learn more      â”‚ average reward         â”‚
            â”‚                 â”‚                        â”‚
            â”‚ Risk: Wasting   â”‚ Risk: Missing a        â”‚
            â”‚ pulls on bad    â”‚ better arm you         â”‚
            â”‚ arms            â”‚ haven't tried enough   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Too much exploration** â†’ You waste time on bad arms  
**Too much exploitation** â†’ You might miss the truly best arm  

The optimal strategy must **balance both**.

---

## 3. Naive Strategies and Their Flaws

### Strategy 1: Pure Exploration (Random)
- Pick a random arm each round
- Problem: Never leverages what you've learned â†’ **Linear regret** O(T)

### Strategy 2: Pure Exploitation (Greedy)
- Always pick the arm with highest sample mean
- Problem: Can get stuck on a suboptimal arm forever â†’ **Linear regret** O(T)

### Strategy 3: Epsilon-Greedy
- With probability `Îµ`: explore (pick random arm)
- With probability `1 - Îµ`: exploit (pick best-so-far arm)
- Problem: `Îµ` is a **hyperparameter** you must tune, and even optimal Îµ gives **linear regret** unless Îµ decays

### What We Want
> An algorithm that **automatically** balances exploration and exploitation,
> with **no hyperparameters** to tune, and achieves **sub-linear** (logarithmic) regret.

**Enter UCB1!** âœ¨

---

## 4. Concentration Inequalities â€” The Mathematical Foundation

Before deriving UCB1, we need **Hoeffding's Inequality** â€” the key mathematical tool.

### The Central Question

> If I've observed `n` samples from a distribution with true mean `Î¼`,
> how far can the sample mean `XÌ„â‚™` be from `Î¼`?

### Law of Large Numbers (Informal)

As `n â†’ âˆ`, the sample mean `XÌ„â‚™ â†’ Î¼` (the true mean).

But we need a **quantitative** version â€” *how fast* does it converge?

### Hoeffding's Inequality

**Theorem (Hoeffding, 1963):** Let `Xâ‚, Xâ‚‚, ..., Xâ‚™` be independent random variables with `Xáµ¢ âˆˆ [0, 1]`. Let `XÌ„â‚™ = (1/n) âˆ‘ Xáµ¢` be the sample mean and `Î¼ = E[XÌ„â‚™]` be the true mean. Then:

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                  â”‚
    â”‚   P( XÌ„â‚™ - Î¼  â‰¥  u )  â‰¤  exp(-2nuÂ²)            â”‚
    â”‚                                                  â”‚
    â”‚   P( Î¼ - XÌ„â‚™  â‰¥  u )  â‰¤  exp(-2nuÂ²)            â”‚
    â”‚                                                  â”‚
    â”‚   Combining (union bound):                       â”‚
    â”‚   P( |XÌ„â‚™ - Î¼| â‰¥  u )  â‰¤  2Â·exp(-2nuÂ²)         â”‚
    â”‚                                                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Does This Mean?

- After `n` samples, the probability that the sample mean is far from the true mean **decreases exponentially** with `n`
- The more samples we have, the **tighter** our estimate

### Building a Confidence Interval

From Hoeffding's inequality, we can construct a confidence interval. If we want:

```
    P( |XÌ„â‚™ - Î¼| â‰¥ u ) â‰¤ Î´
```

Set `Î´ = 2Â·exp(-2nuÂ²)`, then solve for `u`:

```
    2Â·exp(-2nuÂ²) = Î´
    exp(-2nuÂ²) = Î´/2
    -2nuÂ² = ln(Î´/2)
    uÂ² = -ln(Î´/2) / (2n) = ln(2/Î´) / (2n)
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                      â”‚
    â”‚   u = âˆš( ln(2/Î´) / (2n) )           â”‚
    â”‚                                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

So with probability at least `1 - Î´`:

```
    Î¼ âˆˆ [ XÌ„â‚™ - u,  XÌ„â‚™ + u ]    where u = âˆš( ln(2/Î´) / (2n) )
```

**Visual Intuition:**

```
    â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
    
    Few samples (n small):
    â—„â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â–º
                           XÌ„â‚™ Â± u (WIDE interval)
    
    Many samples (n large):
    Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·â—„â”â”â”â”â”â”â”â”â”â”â”â”â”â–ºÂ·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·
                       XÌ„â‚™ Â± u (NARROW interval)
    
    â†’ More samples = more confidence = smaller interval
```

---

## 5. The UCB1 Algorithm

### Core Idea: Optimism in the Face of Uncertainty

> "Give each arm the **benefit of the doubt**."
> 
> Instead of using the sample mean alone, use the **upper** end of the confidence interval.
> Arms we know little about get a **large bonus** (encourages exploration).
> Arms we know a lot about are judged mostly on their **sample mean** (exploitation).

### The UCB1 Formula

For each arm `i` at time step `t`, compute:

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                          â”‚
    â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚                              â”‚ 2Â·ln(t)  â”‚                â”‚
    â”‚   UCBáµ¢(t) = XÌ„áµ¢(t)  +  âˆš  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€ â”‚                â”‚
    â”‚                              â”‚  Náµ¢(t)   â”‚                â”‚
    â”‚              â”€â”€â”€â”€â”€           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
    â”‚              exploit          explore                    â”‚
    â”‚              term             bonus                      â”‚
    â”‚                                                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Where:
    â€¢ XÌ„áµ¢(t) = sample mean of arm i after Náµ¢(t) pulls
    â€¢ Náµ¢(t) = number of times arm i has been pulled up to time t
    â€¢ t     = current time step (total pulls so far)
```

### Why This Works

| Component | What It Does | Behavior |
|-----------|-------------|----------|
| `XÌ„áµ¢(t)` | Estimated reward | Favors arms that **performed well** |
| `âˆš(2Â·ln(t)/Náµ¢(t))` | Exploration bonus | Favors arms **played less often** |
| As `Náµ¢ â†‘` | Bonus shrinks | More exploitation |
| As `Náµ¢` stays small | Bonus stays large | Forces exploration |
| `ln(t)` grows slowly | Bonus grows with time | Ensures all arms are revisited |

### The Algorithm

```
    UCB1 Algorithm
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Input: K arms, T time steps
    
    1. INITIALIZATION:
       For i = 1 to K:
           Play arm i once
           Record reward
       
    2. MAIN LOOP:
       For t = K+1 to T:
           For each arm i = 1 to K:
               Calculate UCBáµ¢(t) = XÌ„áµ¢ + âˆš(2Â·ln(t) / Náµ¢)
           
           Select arm A(t) = argmax_i  UCBáµ¢(t)
           
           Play arm A(t), observe reward r
           Update:
               Nâ‚(t) â† Nâ‚(t) + 1
               XÌ„â‚ â† updated sample mean
    
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Worked Example

Suppose we have **3 arms** and we're at time `t = 100`:

| Arm | Náµ¢ (times played) | XÌ„áµ¢ (avg reward) | Explore Bonus âˆš(2Â·ln(100)/Náµ¢) | UCBáµ¢ |
|-----|------|------|------|------|
| 1 | 60 | 0.72 | âˆš(2Ã—4.605/60) = 0.392 | **1.112** |
| 2 | 35 | 0.68 | âˆš(2Ã—4.605/35) = 0.513 | **1.193** |
| 3 | 5 | 0.50 | âˆš(2Ã—4.605/5) = 1.357  | **1.857** |

**Selected arm: 3** (despite having the lowest average!) â€” because it has huge uncertainty.

After playing arm 3 many more times, its bonus will shrink, and if it's truly bad, arms 1 or 2 will dominate.

---

## 6. The UCB1 Theorem â€” Regret Bound

### What Is Regret?

**Regret** measures how much worse we did compared to always playing the **best arm**.

Let `Î¼* = max_i Î¼áµ¢` be the true mean of the best arm. Then:

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                              â”‚
    â”‚   Cumulative Regret after T rounds:          |
    â”‚                                              â”‚
    â”‚              T                               â”‚
    â”‚   R(T)  =   âˆ‘  (Î¼* - Î¼_{A(t)})              â”‚
    â”‚             t=1                              â”‚
    â”‚                                              â”‚
    â”‚         =   âˆ‘   Î”áµ¢ Â· E[Náµ¢(T)]              â”‚
    â”‚           i: suboptimal                      â”‚
    â”‚                                              â”‚
    â”‚   Where Î”áµ¢ = Î¼* - Î¼áµ¢  (suboptimality gap)   â”‚
    â”‚                                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The UCB1 Theorem (Auer, Cesa-Bianchi, Fischer, 2002)

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                          â”‚
    â”‚   THEOREM (UCB1 Regret Bound):                           â”‚
    â”‚                                                          â”‚
    â”‚   For the UCB1 algorithm with K arms and rewards         â”‚
    â”‚   in [0, 1], the expected cumulative regret after        â”‚
    â”‚   T rounds satisfies:                                    â”‚
    â”‚                                                          â”‚
    â”‚              K     8Â·ln(T)          K                    â”‚
    â”‚   E[R(T)] â‰¤  âˆ‘    â”€â”€â”€â”€â”€â”€â”€ + (1 + Ï€Â²/3) âˆ‘ Î”áµ¢            â”‚
    â”‚             i=1     Î”áµ¢             i=1                   â”‚
    â”‚           iâ‰ i*                     iâ‰ i*                  â”‚
    â”‚                                                          â”‚
    â”‚   This is O(K Â· log(T)) â€” LOGARITHMIC in T!             â”‚
    â”‚                                                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Is This Remarkable?

| Strategy | Regret Growth | Rating |
|----------|--------------|--------|
| Random | O(T) â€” linear | ğŸ˜ Terrible |
| Greedy | O(T) â€” linear | ğŸ˜ Terrible |
| Îµ-Greedy (fixed) | O(T) â€” linear | ğŸ˜ Bad |
| **UCB1** | **O(log T)** â€” logarithmic | ğŸ‰ **Near-optimal** |
| Theoretical lower bound | Î©(log T) | ğŸ“ Unbeatable |

UCB1's regret is only a **constant factor** away from the theoretical best possible!

```
    Regret
    â–²
    â”‚                                              â•± Îµ-Greedy (linear)
    â”‚                                           â•±
    â”‚                                        â•±
    â”‚                                     â•±
    â”‚                                  â•±
    â”‚                               â•±
    â”‚                            â•±
    â”‚        â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„  UCB1 (logarithmic)
    â”‚      â•±
    â”‚    â•±
    â”‚  â•±
    â”‚â•±
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Time (T)
```

---

## 7. Step-by-Step Proof Sketch

### Key Idea

We bound the **expected number of times** each suboptimal arm `i` is pulled: `E[Náµ¢(T)]`.

### Step 1: When Is a Suboptimal Arm Pulled?

Arm `i` (with `Î¼áµ¢ < Î¼*`) is pulled at time `t` only if:

```
    UCBáµ¢(t) â‰¥ UCB*(t)
    
    i.e., XÌ„áµ¢ + âˆš(2ln(t)/Náµ¢) â‰¥ XÌ„* + âˆš(2ln(t)/N*)
```

This can only happen if at least one of these "bad events" occurs:

1. **Arm i's sample mean is too high:** `XÌ„áµ¢ â‰¥ Î¼áµ¢ + cáµ¢` (overestimated)
2. **Best arm's sample mean is too low:** `XÌ„* â‰¤ Î¼* - c*` (underestimated)
3. **Arm i hasn't been pulled enough** (its confidence interval is too wide)

### Step 2: Apply Hoeffding's Inequality

For event 1 (arm i overestimated):
```
    P(XÌ„áµ¢ â‰¥ Î¼áµ¢ + âˆš(2ln(t)/Náµ¢)) â‰¤ exp(-2Â·Náµ¢Â·(2ln(t)/Náµ¢)) = exp(-4ln(t)) = tâ»â´
```

For event 2 (best arm underestimated):
```
    P(XÌ„* â‰¤ Î¼* - âˆš(2ln(t)/N*)) â‰¤ exp(-4ln(t)) = tâ»â´
```

Both probabilities are **polynomially small** in `t`.

### Step 3: Bound E[Náµ¢(T)]

After arm `i` has been pulled at least `m = âŒˆ8Â·ln(T)/Î”áµ¢Â²âŒ‰` times, the confidence interval is narrow enough that UCBáµ¢ < Î¼* (with high probability).

So:
```
    E[Náµ¢(T)] â‰¤ m + âˆ‘_{t=1}^{T} P(bad event at time t)
             â‰¤ 8Â·ln(T)/Î”áµ¢Â² + âˆ‘_{t=1}^{âˆ} 2Â·tâ»â´
             â‰¤ 8Â·ln(T)/Î”áµ¢Â² + Ï€Â²/3
```

(The sum `âˆ‘ 2/tâ´` converges to a constant â‰¤ Ï€Â²/3)

### Step 4: Compute Total Regret

```
    E[R(T)] = âˆ‘áµ¢ Î”áµ¢ Â· E[Náµ¢(T)]
            â‰¤ âˆ‘áµ¢ Î”áµ¢ Â· (8Â·ln(T)/Î”áµ¢Â² + Ï€Â²/3)
            = âˆ‘áµ¢ (8Â·ln(T)/Î”áµ¢ + Î”áµ¢Â·Ï€Â²/3)
            = 8Â·ln(T) Â· âˆ‘áµ¢ 1/Î”áµ¢  +  (Ï€Â²/3) Â· âˆ‘áµ¢ Î”áµ¢
```

This gives us the **O(K Â· log T)** regret bound. âˆ

---

## 8. Python Implementation

See [`ucb_tutorial.py`](ucb_tutorial.py) for the full implementation with:

- `BanditArm` class (Bernoulli rewards)
- `UCB1Agent` with step-by-step logging
- `EpsilonGreedyAgent` for comparison
- `RandomAgent` baseline
- Visualization of regret curves, arm selection frequencies, and confidence intervals

### Quick Start

```bash
cd "d:\MY_WORK\Reinforcement Learning tutorial\ucb_tutorial"
pip install numpy matplotlib
python ucb_tutorial.py
```

---

## 9. Experiments and Visualization

The Python script runs three experiments:

### Experiment 1: Basic UCB1 Behavior
Shows how UCB1 quickly identifies the best arm and allocates most pulls to it.

### Experiment 2: Regret Comparison
Compares cumulative regret of UCB1 vs Epsilon-Greedy vs Random across 10,000 rounds.

### Experiment 3: The Exploration Bonus Over Time
Visualizes how the confidence intervals shrink as more data is collected.

---

## 10. Comparison with Other Strategies

| Feature | UCB1 | Îµ-Greedy | Thompson Sampling |
|---------|------|----------|-------------------|
| Hyperparameters | **None** | Îµ (needs tuning) | Prior distribution |
| Deterministic? | **Yes** | No | No |
| Regret bound | O(log T) | O(T) with fixed Îµ | O(log T) |
| Adapts exploration? | **Automatically** | Fixed rate | Automatically |
| Computational cost | O(K) per step | O(K) per step | O(K) per step |

---

## 11. Key Takeaways

1. **The Multi-Armed Bandit** is the simplest formulation of the explorationâ€“exploitation tradeoff
2. **Hoeffding's Inequality** lets us build confidence intervals around sample means
3. **UCB1** uses these intervals to implement "**optimism in the face of uncertainty**"
4. The **exploration bonus** `âˆš(2Â·ln(t)/Náµ¢)` automatically balances explore vs exploit
5. UCB1 achieves **O(log T) regret** â€” provably near-optimal
6. **No hyperparameters** needed, unlike Îµ-greedy
7. The key insight: arms with **high uncertainty** deserve to be explored, not ignored

---

## References

1. Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). *Finite-time Analysis of the Multiarmed Bandit Problem*. Machine Learning, 47(2-3), 235-256.
2. Lattimore, T. & SzepesvÃ¡ri, C. (2020). *Bandit Algorithms*. Cambridge University Press.
3. Slivkins, A. (2019). *Introduction to Multi-Armed Bandits*. Foundations and Trends in Machine Learning.
4. Hoeffding, W. (1963). *Probability Inequalities for Sums of Bounded Random Variables*. JASA, 58(301), 13-30.
