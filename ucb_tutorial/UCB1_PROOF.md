# UCB1 Regret Bound â€” Complete Step-by-Step Proof

## The Theorem We Will Prove

> **Theorem (Auer, Cesa-Bianchi, Fischer 2002).**
> Consider a K-armed bandit with rewards in [0, 1]. Let Î¼â‚, Î¼â‚‚, â€¦, Î¼_K be the true
> means and Î¼* = max_i Î¼_i. Define Î”_i = Î¼* âˆ’ Î¼_i as the suboptimality gap of arm i.
> Then UCB1 satisfies:
>
> **E[R_T] â‰¤ âˆ‘_{i: Î”_i > 0} (8 ln T)/Î”_i + (1 + Ï€Â²/3) Â· âˆ‘_{i: Î”_i > 0} Î”_i**
>
> This is **O(K Â· log T)** â€” logarithmic regret.

---

## Roadmap

```
Step 0: Notation & Setup
    â†“
Step 1: Hoeffding's Inequality (the key tool)
    â†“
Step 2: Decompose regret into per-arm counts
    â†“
Step 3: Bound when a suboptimal arm is pulled
    â†“
Step 4: Bound E[N_i(T)] for each suboptimal arm
    â†“
Step 5: Sum up to get total regret bound
    â†“
Step 6: Interpret the result
```

---

## Step 0: Notation & Setup

| Symbol | Meaning |
|--------|---------|
| K | Number of arms |
| T | Total time horizon (number of rounds) |
| Î¼_i | True (unknown) mean reward of arm i |
| Î¼* = max_i Î¼_i | Mean of the best arm |
| i* | Index of the best arm (Î¼_{i*} = Î¼*) |
| Î”_i = Î¼* âˆ’ Î¼_i | Suboptimality gap of arm i |
| N_i(t) | Number of times arm i has been pulled through time t |
| XÌ„_i(t) | Sample mean of arm i after N_i(t) pulls: XÌ„_i = (1/N_i) âˆ‘ rewards from arm i |
| A(t) | The arm selected at time t |

**UCB1 rule:** At time t, select arm:

```
A(t) = argmax_i [ XÌ„_i(t-1) + âˆš(2 ln t / N_i(t-1)) ]
```

---

## Step 1: Hoeffding's Inequality

This is the foundational tool. It tells us how far a sample mean can deviate from
the true mean.

### Statement

> Let Xâ‚, Xâ‚‚, â€¦, X_n be i.i.d. random variables with X_j âˆˆ [0, 1] and E[X_j] = Î¼.
> Let XÌ„_n = (1/n) âˆ‘_{j=1}^{n} X_j. Then for any a > 0:
>
> **P(XÌ„_n â‰¥ Î¼ + a) â‰¤ exp(âˆ’2naÂ²)**
>
> **P(XÌ„_n â‰¤ Î¼ âˆ’ a) â‰¤ exp(âˆ’2naÂ²)**

### What this gives us

If we set **a = âˆš(2 ln t / n)**, then:

```
P(XÌ„_n â‰¥ Î¼ + âˆš(2 ln t / n)) â‰¤ exp(âˆ’2n Â· (2 ln t / n))
                              = exp(âˆ’4 ln t)
                              = t^{âˆ’4}
```

So after n pulls, the probability that the sample mean overshoots the true mean
by more than âˆš(2 ln t / n) is at most **1/tâ´** â€” extremely small.

Similarly:

```
P(XÌ„_n â‰¤ Î¼ âˆ’ âˆš(2 ln t / n)) â‰¤ t^{âˆ’4}
```

This is why UCB1 uses âˆš(2 ln t / N_i) as the confidence radius â€” it makes
"bad events" happen with probability at most 1/tâ´.

---

## Step 2: Decompose Regret into Per-Arm Counts

The cumulative regret after T rounds is:

```
R_T = âˆ‘_{t=1}^{T} (Î¼* âˆ’ Î¼_{A(t)})
```

We can rewrite this by grouping by which arm was played:

```
R_T = âˆ‘_{i=1}^{K} Î”_i Â· N_i(T)
```

**Why?** Each time suboptimal arm i is played, we lose Î”_i = Î¼* âˆ’ Î¼_i in expected
reward. If arm i is played N_i(T) times total, the total loss from arm i is Î”_i Â· N_i(T).

Taking expectations:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                â”‚
â”‚   E[R_T] = âˆ‘_{i: Î”_i > 0}  Î”_i Â· E[N_i(T)]   â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key insight:** To bound the total regret, we just need to bound **E[N_i(T)]** â€” the
expected number of times each suboptimal arm i is pulled.

---

## Step 3: When Is a Suboptimal Arm Pulled?

A suboptimal arm i (with Î”_i > 0) is pulled at time t only if its UCB is the highest:

```
UCB_i(t) â‰¥ UCB_{i*}(t)
```

Expanding:

```
XÌ„_i + âˆš(2 ln t / N_i) â‰¥ XÌ„_{i*} + âˆš(2 ln t / N_{i*})
```

Define the confidence radii:

```
c_{i,s}(t) = âˆš(2 ln t / s)     â€” radius when arm i has been pulled s times
```

For arm i to be pulled, at least one of three things must be true. We label
them as events:

---

### Event Eâ‚: The best arm's sample mean is too low

```
Eâ‚: XÌ„_{i*} â‰¤ Î¼* âˆ’ c_{i*, N_{i*}}(t)
```

The best arm's sample mean has fallen **below** its lower confidence bound.
By Hoeffding: **P(Eâ‚) â‰¤ t^{âˆ’4}**

---

### Event Eâ‚‚: The suboptimal arm's sample mean is too high

```
Eâ‚‚: XÌ„_i â‰¥ Î¼_i + c_{i, N_i}(t)
```

Arm i's sample mean has risen **above** its upper confidence bound.
By Hoeffding: **P(Eâ‚‚) â‰¤ t^{âˆ’4}**

---

### Event Eâ‚ƒ: Arm i hasn't been pulled enough

```
Eâ‚ƒ: N_i(t) < â„“    where â„“ = âŒˆ(8 ln T) / Î”_iÂ²âŒ‰
```

If arm i has been pulled fewer than â„“ times, its confidence interval is still
wide enough that UCB_i could exceed UCB_{i*} even when Eâ‚ and Eâ‚‚ don't occur.

---

### Why these three events cover everything

**Claim:** If Eâ‚, Eâ‚‚, and Eâ‚ƒ all fail (i.e., none of them occur), then arm i
**cannot** be pulled.

**Proof of claim:**

Assume Â¬Eâ‚, Â¬Eâ‚‚, Â¬Eâ‚ƒ all hold. Then:

1. Â¬Eâ‚ means: XÌ„_{i*} > Î¼* âˆ’ c_{i*, N_{i*}}(t)
   â†’ **UCB_{i*} = XÌ„_{i*} + c_{i*, N_{i*}}(t) > Î¼***
   (actually, UCB_{i*} â‰¥ XÌ„_{i*} + c > Î¼* âˆ’ c + c = Î¼*)

2. Â¬Eâ‚‚ means: XÌ„_i < Î¼_i + c_{i, N_i}(t)
   â†’ **UCB_i = XÌ„_i + c_{i, N_i}(t) < Î¼_i + 2Â·c_{i, N_i}(t)**

3. Â¬Eâ‚ƒ means: N_i â‰¥ â„“ = âŒˆ(8 ln T) / Î”_iÂ²âŒ‰
   â†’ c_{i, N_i}(t) = âˆš(2 ln t / N_i) â‰¤ âˆš(2 ln T / â„“)

   Now substitute â„“ = 8 ln T / Î”_iÂ²:
   ```
   c_{i, N_i}(t) â‰¤ âˆš(2 ln T / (8 ln T / Î”_iÂ²))
                   = âˆš(2 Â· Î”_iÂ² / 8)
                   = âˆš(Î”_iÂ² / 4)
                   = Î”_i / 2
   ```

Combining 2 and 3:
```
UCB_i < Î¼_i + 2 Â· (Î”_i / 2) = Î¼_i + Î”_i = Î¼*
```

But from 1: UCB_{i*} â‰¥ Î¼*

Therefore: **UCB_{i*} â‰¥ Î¼* > UCB_i**, so the algorithm picks i* over i. âˆ

---

## Step 4: Bound E[N_i(T)]

We now count how many times suboptimal arm i can be pulled. We decompose:

```
N_i(T) = âˆ‘_{t=1}^{T} ğŸ™{A(t) = i}
```

From Step 3, at each time t, arm i is pulled only if Eâ‚ âˆ¨ Eâ‚‚ âˆ¨ Eâ‚ƒ occurs.

### Part A: Contribution from Eâ‚ƒ (not enough pulls)

Arm i can be pulled at most **â„“ = âŒˆ(8 ln T) / Î”_iÂ²âŒ‰** times due to this event
(after â„“ pulls, Eâ‚ƒ no longer holds).

This contributes at most:

```
âŒˆ(8 ln T) / Î”_iÂ²âŒ‰
```

### Part B: Contribution from Eâ‚ or Eâ‚‚ (bad concentration events)

Even after arm i has been pulled â„“ times, it could still be pulled if Eâ‚ or Eâ‚‚
occurs. We bound the expected number of such occurrences.

At time t, using union bound over all possible values of N_i and N_{i*}:

```
P(arm i pulled at time t, and N_i â‰¥ â„“)
    â‰¤ P(Eâ‚ at time t) + P(Eâ‚‚ at time t)
    â‰¤ âˆ‘_{s=1}^{t} P(XÌ„_{i*,s} â‰¤ Î¼* âˆ’ âˆš(2 ln t / s))
      + âˆ‘_{s=â„“}^{t} P(XÌ„_{i,s} â‰¥ Î¼_i + âˆš(2 ln t / s))
```

But we can bound this more directly. For each time t and each possible
count s (number of pulls of arm i) and s' (number of pulls of best arm):

```
P(Eâ‚ or Eâ‚‚) â‰¤ âˆ‘_{s=1}^{t-1} âˆ‘_{s'=1}^{t-1} [P(XÌ„_{i*,s'} â‰¤ Î¼* âˆ’ c_{s'}(t)) 
                                                  + P(XÌ„_{i,s} â‰¥ Î¼_i + c_s(t))]
```

By Hoeffding (from Step 1), each probability is at most t^{âˆ’4}, so:

```
P(Eâ‚ or Eâ‚‚) â‰¤ âˆ‘_{s=1}^{t-1} âˆ‘_{s'=1}^{t-1} 2Â·t^{âˆ’4}
             â‰¤ tÂ² Â· 2Â·t^{âˆ’4}
             = 2Â·t^{âˆ’2}
```

### Summing over all time steps

```
Total contribution from Eâ‚, Eâ‚‚ = âˆ‘_{t=1}^{âˆ} 2Â·t^{âˆ’2}
                                = 2 Â· Ï€Â²/6
                                = Ï€Â²/3
```

(We use the Basel series: âˆ‘_{t=1}^{âˆ} 1/tÂ² = Ï€Â²/6.)

### Combining Parts A and B

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                       â”‚
â”‚   E[N_i(T)] â‰¤ âŒˆ(8 ln T) / Î”_iÂ²âŒ‰ + Ï€Â²/3             â”‚
â”‚                                                       â”‚
â”‚   Simplifying the ceiling:                            â”‚
â”‚   E[N_i(T)] â‰¤ (8 ln T) / Î”_iÂ² + 1 + Ï€Â²/3            â”‚
â”‚                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Step 5: Compute Total Regret

Plugging back into the regret decomposition from Step 2:

```
E[R_T] = âˆ‘_{i: Î”_i > 0} Î”_i Â· E[N_i(T)]

       â‰¤ âˆ‘_{i: Î”_i > 0} Î”_i Â· [(8 ln T) / Î”_iÂ² + 1 + Ï€Â²/3]

       = âˆ‘_{i: Î”_i > 0} [(8 ln T) / Î”_i + Î”_i Â· (1 + Ï€Â²/3)]
```

Separating the two sums:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚                        K                        K            â”‚
â”‚   E[R_T]  â‰¤   8 ln T Â· âˆ‘  (1/Î”_i)  +  (1+Ï€Â²/3) âˆ‘  Î”_i     â”‚
â”‚                       i=1                      i=1           â”‚
â”‚                      iâ‰ i*                     iâ‰ i*           â”‚
â”‚                                                              â”‚
â”‚           =  O(K Â· log T)                                    â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚   This is the UCB1 Theorem. âˆ                                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Step 6: Understanding the Result

### Term by term

| Term | What it means |
|------|--------------|
| 8 ln T / Î”_i | The "necessary exploration" cost. Arms close to optimal (small Î”_i) are hard to distinguish â†’ more pulls needed. |
| (1 + Ï€Â²/3) Â· Î”_i | A small constant cost from the rare "bad events" (concentration failures). Bounded by a constant. |

### Growth rate

```
   Regret
     â–²
     â”‚
 800 â”¤                                          â•± Linear O(T) â€” bad algorithms
     â”‚                                       â•±
 600 â”¤                                    â•±
     â”‚                                 â•±
 400 â”¤                              â•±
     â”‚                           â•±
 200 â”¤        ............................  Logarithmic O(log T) â€” UCB1
     â”‚      .Â·
 100 â”¤    .Â·
     â”‚  .Â·
     â”‚.Â·
     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â–º T
          2000   4000   6000   8000  10000
```

- **Log T grows MUCH slower than T**. After 10,000 rounds, log(10000) â‰ˆ 9.2.
- This means the **average regret per round â†’ 0** as T â†’ âˆ.
- UCB1 is **consistent**: it eventually figures out the best arm.

### Is this the best possible?

Yes (up to constants)! The **Lai-Robbins lower bound (1985)** proves:

> For any algorithm, E[R_T] â‰¥ âˆ‘_i (Î”_i / KL(Î¼_i, Î¼*)) Â· ln T

where KL(Â·,Â·) is the KL divergence. Since KL(Î¼_i, Î¼*) â‰¤ 2/Î”_iÂ² for bounded
rewards, this gives a **Î©(log T)** lower bound.

UCB1's O(log T) **matches this lower bound** in order â€” it's *rate-optimal*.

---

## Summary of the Proof Flow

```
Hoeffding's Inequality
    â”‚
    â–¼
"Bad events" (Eâ‚, Eâ‚‚) happen with probability â‰¤ t^{âˆ’4}
    â”‚
    â–¼
After â„“ = O(ln T / Î”_iÂ²) pulls, arm i's UCB < Î¼* (when no bad events)
    â”‚
    â–¼
E[N_i(T)] â‰¤ 8 ln T / Î”_iÂ²  +  constant
    â”‚
    â–¼
E[R_T] = âˆ‘ Î”_i Â· E[N_i(T)] â‰¤ 8 ln T Â· âˆ‘ 1/Î”_i  +  constant
    â”‚
    â–¼
Regret = O(K Â· log T)    âœ“
```

---

## Conditions Under Which UCB1 Works

The proof above relies on specific assumptions. Here is exactly where each one
is used, and what happens when it breaks.

### Assumption 1: Bounded Rewards â€” X_t âˆˆ [0, 1]

- **Used in:** Step 1 (Hoeffding's Inequality requires bounded random variables)
- **If violated:** The concentration bound exp(âˆ’2naÂ²) no longer holds. With
  unbounded or heavy-tailed rewards, the sample mean converges much slower.
- âŒ **Breaks for:** Gaussian with unknown variance, Pareto, log-normal rewards
- âœ… **Fix:** If rewards are in [a, b], rescale via r' = (r âˆ’ a)/(b âˆ’ a). For
  truly unbounded rewards, use **Robust UCB** or **Median-of-Means UCB**.

### Assumption 2: Stationarity â€” Î¼_i is constant over time

- **Used in:** Step 2 (regret decomposition assumes Î¼* is fixed) and Step 4
  (sample mean XÌ„_i converges to a fixed Î¼_i)
- **If violated:** The sample mean converges to a time-average, not the current
  mean. The best arm might change, making past data misleading.
- âŒ **Breaks for:** Trending rewards, seasonal patterns, adversarial settings
- âœ… **Fix:** **Sliding Window UCB** (only use recent data), **Discounted UCB**
  (exponentially weight recent observations), or **EXP3** (adversarial setting).

### Assumption 3: Independence â€” rewards are i.i.d. within each arm

- **Used in:** Step 1 (Hoeffding requires Xâ‚, Xâ‚‚, â€¦, Xâ‚™ to be independent)
- **If violated:** The concentration rate changes. Positively correlated samples
  make the sample mean converge slower than 1/âˆšn.
- âŒ **Breaks for:** Time-series data, correlated user sessions, sequential trials
  with carryover effects
- âœ… **Fix:** Use modified concentration inequalities for dependent data
  (e.g., martingale-based bounds), or **restless bandits** formulations.

### Assumption 4: No Context â€” the optimal arm is the same in every round

- **Used in:** Step 2 (single Î¼* for all rounds)
- **If violated:** The best arm depends on a feature vector x_t (e.g., user
  demographics). A single global ranking of arms is meaningless.
- âŒ **Breaks for:** Personalized recommendations, user-specific ad targeting
- âœ… **Fix:** **LinUCB** (linear contextual bandits), **Kernel UCB**, or
  **Contextual Thompson Sampling**.

### Assumption 5: Finite, Fixed Arm Set â€” K arms known in advance

- **Used in:** Step 0 (initialization requires playing each arm once) and Step 4
  (sum over i = 1 to K)
- **If violated:** If K is very large or infinite, initialization alone costs too
  much. If arms appear/disappear, the indexing scheme breaks.
- âŒ **Breaks for:** Continuous action spaces, dynamically changing action sets
- âœ… **Fix:** **GP-UCB** (Gaussian Process UCB) for continuous arms,
  **Combinatorial bandits** for exponentially large arm sets.

### Assumption 6: Bandit Feedback â€” you observe reward of the chosen arm only

- **Used in:** Algorithm definition (update rule only sees one reward per round)
- This is NOT a restrictive assumption â€” it's the *definition* of the bandit
  setting. UCB1 does NOT need to see what other arms would have given.
- âœ… **If you observe all arms' rewards:** You're in the **full information**
  setting, which is strictly easier. UCB1 still works but is overkill.

### Quick Reference Table

| Assumption | Where Used in Proof | What Breaks | Use Instead |
|-----------|-------|-------|------------|
| Bounded [0,1] rewards | Hoeffding (Step 1) | Concentration bound | Robust UCB, Median-of-Means |
| Stationary means | Regret decomp (Step 2) | Sample mean misleading | Sliding Window UCB, EXP3 |
| i.i.d. rewards | Hoeffding (Step 1) | Convergence rate wrong | Martingale bounds |
| No context/state | Single Î¼* (Step 2) | Best arm varies | LinUCB, Contextual bandits |
| Finite fixed K arms | Initialization (Step 0) | Can't try all arms | GP-UCB, Continuum bandits |
| Bandit feedback | Update rule | N/A (not restrictive) | Still works with more info |

---

## Reference

Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). *Finite-time Analysis of the
Multiarmed Bandit Problem.* Machine Learning, 47(2-3), 235â€“256.
